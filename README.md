# pi
pi stands for prompt injector.

I want to build a tool like SQLmap to pentest the prompt of LLMs.

There are lots of errors when I try to use [llm-attacks](https://github.com/llm-attacks/llm-attacks) in other LLM.

I'm so depressed by the existing tools. They don't match the various LLMs in HuggingFace.

Why don't create a tool so we can choose and add different LLM and prompt injection method easily?

GO!
